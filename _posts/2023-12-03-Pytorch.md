---
title: Pytorch
tags: Machine_Learnig
typora-root-url: ../
---

# 1. 파이토치(Pytorch)
* tensorflow와 함께 머신러닝, 딥런닝에서 가장 널리 사용되는 프레임워크
* 초기에는 Torch라는 이름으로, Lua언어 기반으로 만들어졌으나, 이후 Python 기반으로 변경한 것이 Pytorch
* 뉴욕 대학교와 페이스북이 공동으로 개발하였고, 현재 가장 대중적이고 널리 사용되는 프레임워크

!pip install tensorflow # GPU버전
pip install tensorflow-cpu # cpu버전


```python
import torch
print(torch.__version__)
```

    2.0.0+cu117
    

### 1-1. Tensor
* 텐서는 배열이나 행렬과 매우 유사한 특수 자료구조
* 파이토치는 텐서를 사용하여 모델의 입력과 출력, 모델의 매개변수들을 부호화함


```python
data = [[1,2],[3,4]]
x_data = torch.tensor(data)
print(x_data)
```

    tensor([[1, 2],
            [3, 4]])
    


```python
import numpy as np
```


```python
np_array = np.array(data)
x_np_1 = torch.tensor(np_array)
print(x_np_1)
```

    tensor([[1, 2],
            [3, 4]], dtype=torch.int32)
    


```python
#tensorflow와 달리 값 대입 가능
x_np_1[0,0]=100
print(x_np_1) # 새로운 텐서를 생성해 값 저장
print(np_array) # 기존 ndarray
```

    tensor([[100,   2],
            [  3,   4]], dtype=torch.int32)
    [[1 2]
     [3 4]]
    


```python
x_np_2 = torch.as_tensor(np_array)
print(x_np_2)
```

    tensor([[1, 2],
            [3, 4]], dtype=torch.int32)
    


```python
x_np_2[0,0] = 200
print(x_np_2) # 동일한 메모리 주소를 가리키는 뷰를 만듬
print(np_array) # 기존 메모리 주소의 ndarray값이 변경됨
```

    tensor([[200,   2],
            [  3,   4]], dtype=torch.int32)
    [[200   2]
     [  3   4]]
    


```python
x_np_3 = torch.from_numpy(np_array)
print(x_np_3)
```

    tensor([[200,   2],
            [  3,   4]], dtype=torch.int32)
    


```python
x_np_3[0,0]=300
print(x_np_3) # 동일한 메모리 주소를 가리키는 뷰를 만듬
print(np_array) # 기존 메모리 주소의 ndarray값이 변경됨
```

    tensor([[300,   2],
            [  3,   4]], dtype=torch.int32)
    [[300   2]
     [  3   4]]
    


```python
np_again = x_np_1.numpy()
print(np_again,type(np_again))
```

    [[100   2]
     [  3   4]] <class 'numpy.ndarray'>
    


```python
a = torch.ones(2,3)
print(a)
b= torch.zeros(2,3)
print(b)

c= torch.full((2,3),2)
print(c)

d= torch.empty(2,3) # 아무 값이나
print(d)

e = torch.eye(5)
print(e)

f= torch. arange(10)
print(f)

g= torch.rand(2,2)
print(g)

h= torch.randn(2,2)
print(h)
```

    tensor([[1., 1., 1.],
            [1., 1., 1.]])
    tensor([[0., 0., 0.],
            [0., 0., 0.]])
    tensor([[2, 2, 2],
            [2, 2, 2]])
    tensor([[0.0000e+00, 0.0000e+00, 1.8754e+28],
            [2.0110e+20, 5.4173e-05, 1.0482e-11]])
    tensor([[1., 0., 0., 0., 0.],
            [0., 1., 0., 0., 0.],
            [0., 0., 1., 0., 0.],
            [0., 0., 0., 1., 0.],
            [0., 0., 0., 0., 1.]])
    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    tensor([[0.1229, 0.7767],
            [0.9390, 0.0613]])
    tensor([[ 0.0428, -0.5715],
            [ 0.1872,  0.1023]])
    

### 1-2 .Tensor 속성


```python
tensor = torch.rand(3,4)
print(f'shape: {tensor.shape}')
print(f'shape: {tensor.dtype}')
print(f'device:{tensor.device}')

```

    shape: torch.Size([3, 4])
    shape: torch.float32
    device:cpu
    

### 1-3. GPU 사용하기
* 코랩에서 device 변경하는 방법
    * 상단 메뉴 -> 런타임 -> 런타임 유형 변경 -> 하드웨어 가속기를 GPU로 변경 -> 런타임 다시 시작 및 모두 실행


```python
tensor = tensor.reshape(4,3)
tensor = tensor.int()
if torch.cuda.is_available(): #gpu를 사용할 수 있는지 여부
    tensor = tensor.to('cuda')
    
print(f'shape: {tensor.shape}')
print(f'shape: {tensor.dtype}')
print(f'device:{tensor.device}') # gpu로 넘어갔으면 cuda로 변경됨

```

    shape: torch.Size([4, 3])
    shape: torch.int32
    device:cpu
    

### 1-4 . Indexing과 Slicing


```python
a = torch.arange(1,13).reshape(3,4)
print(a)
```

    tensor([[ 1,  2,  3,  4],
            [ 5,  6,  7,  8],
            [ 9, 10, 11, 12]])
    


```python
print(a[1])
print(a[0,-1])
print(a[1:-1])
print(a[:2,2:])

```

    tensor([5, 6, 7, 8])
    tensor(4)
    tensor([[5, 6, 7, 8]])
    tensor([[3, 4],
            [7, 8]])
    

### 1-5. Transpose
* 2개의 차원을 맞교환할 때 사용


```python
a= torch.arange(16).reshape(2,2,4)
print(a,a.shape)
```

    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7]],
    
            [[ 8,  9, 10, 11],
             [12, 13, 14, 15]]]) torch.Size([2, 2, 4])
    


```python
b= a.transpose(1,2) # parameter로 2개밖에 사용 못함( 나머지 하나는 자동으로 배정 )
print(b,b.shape)
```

    tensor([[[ 0,  4],
             [ 1,  5],
             [ 2,  6],
             [ 3,  7]],
    
            [[ 8, 12],
             [ 9, 13],
             [10, 14],
             [11, 15]]]) torch.Size([2, 4, 2])
    

### 1-6. permute


```python
c = a.permute((2,0,1)) #[2,2,4] -> [4,2,2]
print(c)
```

    tensor([[[ 0,  4],
             [ 8, 12]],
    
            [[ 1,  5],
             [ 9, 13]],
    
            [[ 2,  6],
             [10, 14]],
    
            [[ 3,  7],
             [11, 15]]])
    

### 1-7. Tensor 연산


```python
x= torch.tensor([[1,2],[3,4]],dtype = torch.float32)
y= torch.tensor([[1,2],[3,4]],dtype = torch.float32)
print(x)
print(y)
```

    tensor([[1., 2.],
            [3., 4.]])
    tensor([[1., 2.],
            [3., 4.]])
    


```python
print(torch.add(x,y))
print(torch.subtract(x,y))
print(torch.multiply(x,y))
print(torch.divide(x,y))
print(torch.matmul(x,y)) # 내적
```

    tensor([[2., 4.],
            [6., 8.]])
    tensor([[0., 0.],
            [0., 0.]])
    tensor([[ 1.,  4.],
            [ 9., 16.]])
    tensor([[1., 1.],
            [1., 1.]])
    tensor([[ 7., 10.],
            [15., 22.]])
    


```python
print(x + y)
print(x - y)
print(x * y)
print(x / y)
print(x @ y) # 내적
```

    tensor([[2., 4.],
            [6., 8.]])
    tensor([[0., 0.],
            [0., 0.]])
    tensor([[ 1.,  4.],
            [ 9., 16.]])
    tensor([[1., 1.],
            [1., 1.]])
    tensor([[ 7., 10.],
            [15., 22.]])
    


```python
# in-place 연산
# num = num + 1
print(x.add(y))
print(x)
```

    tensor([[2., 4.],
            [6., 8.]])
    tensor([[1., 2.],
            [3., 4.]])
    


```python
print(x.add_(y)) # x에 결과를 다시 저장(_ 붙이면)
print(x)
# 위 모든 연산자에 _를 붙여 사용
```

    tensor([[2., 4.],
            [6., 8.]])
    tensor([[2., 4.],
            [6., 8.]])
    


```python
z = torch.arange(1,11).reshape(2,5)
sum1 = torch.sum(z,axis=0) # 행
sum2 = torch.sum(z,axis=1) # 열
sum3 = torch.sum(z,axis=-1) # 열
print(sum1, sum1.shape)
print(sum2, sum2.shape)
print(sum3, sum3.shape)

```

    tensor([ 7,  9, 11, 13, 15]) torch.Size([5])
    tensor([15, 40]) torch.Size([2])
    tensor([15, 40]) torch.Size([2])
    


```python
# 행렬 복사+ 붙여넣기
a= torch.arange(24).reshape(4,6)
b= a.clone().detach() # clrl c+ cntl+ v 느낌
print(a,a.shape)
print(b,b.shape)
#둘은 서로 다른 메모리 주소 가리킴
```

    tensor([[ 0,  1,  2,  3,  4,  5],
            [ 6,  7,  8,  9, 10, 11],
            [12, 13, 14, 15, 16, 17],
            [18, 19, 20, 21, 22, 23]]) torch.Size([4, 6])
    tensor([[ 0,  1,  2,  3,  4,  5],
            [ 6,  7,  8,  9, 10, 11],
            [12, 13, 14, 15, 16, 17],
            [18, 19, 20, 21, 22, 23]]) torch.Size([4, 6])
    


```python

```
